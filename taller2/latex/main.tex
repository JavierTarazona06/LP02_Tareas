\documentclass{article}
\usepackage{csquotes}
\usepackage[utf8x]{inputenc}
\usepackage[T1,T2A]{fontenc}
\usepackage{fvextra}
\fvset{commandchars=\\\{\}, mathescape=true}
\usepackage{xcolor}
\usepackage{amssymb}       % define \Sha
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage[style=ieee]{biblatex} % Establecer el estilo de las referencias como IEEE
\usepackage{hyperref}
\usepackage{titletoc}
\usepackage{adjustbox}

\definecolor{bg}{rgb}{0.95,0.95,0.95}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{purple}{rgb}{0.58,0,0.82}
\definecolor{bluekeyword}{rgb}{0.26,0.44,0.76}
\definecolor{lightorange}{rgb}{0.8,0.5,0.2}
\definecolor{darkgreen}{rgb}{0.0,0.5,0.0}

\lstdefinestyle{mypython}{
    inputencoding=utf8,           % Indica a listings que use UTF-8
    extendedchars=true,           % Permite caracteres extendidos
    mathescape=true,              % Habilita modo matemático dentro de literales
    language=Python,
    backgroundcolor=\color{bg},
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{bluekeyword}\bfseries,
    commentstyle=\color{gray}\itshape,
    stringstyle=\color{lightorange},
    numberstyle=\tiny\color{gray},
    identifierstyle=\color{black},
    showstringspaces=false,
    numbers=left,
    numbersep=10pt,
    frame=single,
    breaklines=true,
    tabsize=4,
    captionpos=b,
    escapeinside={(*@}{@*)},
    literate=
     {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
     {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
     {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {π}{{$\pi$}}1  {Ш}{{\fontencoding{T2A}\selectfont\char"DB}}1
     {¡}{{\textexclamdown}}1 {¿}{{\textquestiondown}}1
}


\hypersetup{
    colorlinks=true,
    linkcolor=blue, % Color del texto del enlace
    urlcolor=blue % Color del enlace
}

\usepackage{longtable} % Agrega el paquete longtable

\definecolor{mygreen}{RGB}{0,128,0}

\usepackage{array} % Para personalizar la tabla
\usepackage{booktabs} % Para líneas horizontales de mejor calidad
\usepackage{graphicx} % Paquete para incluir imágenes
\usepackage{float}
\usepackage[section]{placeins}

% Definir márgenes
\usepackage[margin=1in]{geometry}

\renewcommand{\contentsname}{\textcolor{mygreen}{Tabla de Contenidos}}

\begin{document}

\begin{titlepage}
  \centering
  % Logo de la Universidad
  \includegraphics[width=0.48\textwidth]{logo_universidad.png}
  \par\vspace{2cm}

  % Nombre de la Universidad y detalles del curso
  {\Large \textbf{Universidad Nacional de Colombia} \par}
  \vspace{0.5cm}
  {\large Ingeniería de Sistemas y Computación \par}
  {\large 2025966 Lenguajes de Programación (02)\par}
  \vspace{3cm}

  % Detalles del laboratorio y actividad
  {\large \textbf{Taller 2} \par}
  {\large Analizador Sintáctico y Semántico\par}
  \vspace{3cm}

  % Lista de integrantes
  {\large \textbf{Integrantes:} \par}
  \vspace{0.5cm}
  \begin{tabular}{ll}
    Javier Andrés Tarazona Jiménez   & jtarazonaj@unal.edu.co \\
    David Felipe Marin Rosas         & dmarinro@unal.edu.co   \\
    Juan Sebastian Muñoz Lemus       & jumunozle@unal.edu.co          \\
    Eder José Hernández Buelvas      & ehernandezbu@unal.edu.co          \\
    Axel Gomez Moreno                & axgomezm@unal.edu.co          \\
    Daniel Santiago Delgado Pinilla  & ddelgadopi@unal.edu.co          \\
  \end{tabular}
  \par\vspace{3cm}

  % Fecha
  {\large Junio 22 de 2025 \par}
\end{titlepage}

\tableofcontents % Inserta la tabla de contenidos

\newpage % Salto de página para separar la tabla de contenidos del contenido del documento

% Contenido del artículo----------------------------------------------------------

%---------------------------------------------------------------------------------
% Intro --------------------------------------------------------------------------
%---------------------------------------------------------------------------------

\section{Introducción}\label{sec:intr}

En la actualidad, el análisis estadístico desempeña un papel fundamental en la toma de decisiones dentro de diversos campos de la investigación científica y aplicada. La correcta interpretación de datos experimentales, especialmente aquellos organizados en estructuras de clasificación de dos vías, requiere herramientas especializadas que permitan automatizar cálculos complejos y minimizar el margen de error humano.

El presente trabajo tiene como objetivo principal el diseño y construcción de un lenguaje de programación de propósito específico, enfocado en la aplicación de pruebas estadísticas de tipo Friedman y en el análisis de rachas, facilitando la ejecución de procedimientos estadísticos robustos. Este lenguaje se desarrollará bajo el paradigma imperativo, incorporando estructuras de control básicas y soporte para tipos de datos abstractos como variables, arreglos y matrices.

Para lograr este propósito, se propone una solución integral que abarca desde la definición formal de la gramática del lenguaje en notación E-BNF, el diseño de diagramas de sintaxis, la descripción semántica de cada producción, hasta la implementación práctica del compilador utilizando herramientas como FLEX para el análisis léxico y YACC para el análisis sintáctico y la ejecución de acciones semánticas.

Este proyecto no solo contribuye a resolver una necesidad específica del ámbito estadístico, sino que también permite a los estudiantes afianzar sus conocimientos en teoría de lenguajes, compiladores y programación, integrando aspectos teóricos y prácticos en un caso de aplicación real.
%---------------------------------------------------------------------------------
% Marco Teórico ------------------------------------------------------------------
%---------------------------------------------------------------------------------

\section{Marco Teórico}

\subsection*{Lenguajes de Programación de Propósito Específico}

Los lenguajes de programación de propósito específico (DSL, por sus siglas en inglés: \textit{Domain Specific Language}) son lenguajes diseñados para resolver problemas particulares dentro de un dominio limitado de aplicación. A diferencia de los lenguajes de propósito general (GPL), que ofrecen herramientas amplias para múltiples tipos de problemas, los DSL permiten expresar soluciones de manera más directa y eficiente dentro de un campo de especialidad. En el contexto del presente taller, se propone el diseño de un lenguaje de propósito específico orientado al análisis estadístico de dos vías de clasificación, haciendo uso de conceptos como las rachas y las pruebas estadísticas de Friedman.

\subsection*{Clasificación de Dos Vías y Pruebas de Friedman}

La clasificación de dos vías es una técnica estadística utilizada para analizar datos que pueden organizarse en una tabla de doble entrada, considerando dos factores de clasificación. Esta metodología es frecuente en el diseño de experimentos y el análisis no paramétrico, donde se evalúa la influencia de dos factores independientes sobre una variable de respuesta.

Dentro de este marco, la prueba de Friedman es una prueba no paramétrica empleada para detectar diferencias significativas entre grupos relacionados. Es especialmente útil como alternativa a la prueba ANOVA de medidas repetidas, cuando no se cumplen los supuestos de normalidad o igualdad de varianzas. La prueba de Friedman se basa en la clasificación de las observaciones y el cálculo de estadísticos de rachas y rangos para establecer la significancia de las diferencias encontradas.

\subsection*{Concepto de Rachas}

El análisis de rachas es un procedimiento estadístico que evalúa la secuencia de datos para identificar patrones de agrupamiento o alternancia. Una racha se define como una sucesión de resultados similares dentro de una secuencia. Por ejemplo, en una serie de clasificaciones o rangos, las rachas permiten identificar tendencias sistemáticas o aleatoriedad. Este concepto es relevante para validar la independencia de las observaciones o para detectar sesgos en la clasificación de datos.

\subsection*{Gramáticas Libres de Contexto (GLC) y Notación E-BNF}

El diseño de lenguajes de programación requiere la definición formal de su sintaxis mediante gramáticas formales. En este taller se propone el uso de gramáticas libres de contexto (GLC) expresadas en notación E-BNF (\textit{Extended Backus-Naur Form}). Esta notación extiende la notación BNF tradicional para describir de forma más legible y compacta estructuras repetitivas, opciones y secuencias dentro de una gramática.

Una GLC está compuesta por:
\begin{itemize}
    \item Un conjunto finito de símbolos terminales (tokens).
    \item Un conjunto finito de símbolos no terminales.
    \item Un símbolo de inicio.
    \item Un conjunto finito de reglas de producción que definen cómo los símbolos no terminales pueden ser reemplazados por secuencias de símbolos terminales y no terminales.
\end{itemize}

\subsection*{Análisis Léxico y Sintáctico}

El análisis léxico corresponde a la fase del compilador responsable de dividir el texto fuente en tokens significativos, eliminando espacios en blanco y comentarios. En este taller, se hará uso de la herramienta \texttt{FLEX} para construir el analizador léxico. 

El análisis sintáctico, por su parte, valida que la secuencia de tokens se ajuste a las reglas definidas por la gramática, construyendo un árbol de derivación o árbol sintáctico. Para este fin se utilizará \texttt{YACC}, una herramienta para generar analizadores sintácticos basados en gramáticas libres de contexto.

\subsection*{Semántica y Acciones Asociadas}

Además de la sintaxis, un lenguaje de programación requiere definir el significado de sus construcciones. La semántica describe el comportamiento interno del compilador o intérprete al reconocer patrones sintácticos específicos. En este proyecto, se describirá la semántica de cada producción mediante acciones en lenguaje natural y se diseñará su estructura en diagramas UML.

\subsection*{Estructuras de Control y Tipos de Datos Abstractos}

El lenguaje propuesto debe soportar estructuras básicas del paradigma imperativo: secuencia, selección (estructuras condicionales) e iteración (bucles). Asimismo, debe permitir la definición y manipulación de variables, arreglos y matrices como tipos de datos abstractos que faciliten la implementación de procedimientos estadísticos complejos.

\subsection*{Implementación en YACC}

Finalmente, la implementación se realizará mediante \texttt{YACC} (Yet Another Compiler Compiler) o su equivalente en Java o Python. Esto permitirá no solo verificar la validez sintáctica de los programas escritos en el nuevo lenguaje, sino también ejecutar las acciones semánticas requeridas para procesar datos estadísticos y producir resultados coherentes con los métodos de clasificación de dos vías y pruebas de Friedman.

\subsection*{Pruebas y Validación}

Como parte del proceso de validación, se desarrollarán programas de prueba que demuestren la cobertura y robustez de la gramática diseñada, así como la correcta implementación de sus acciones semánticas. Las pruebas de escritorio verificarán que los programas sean aceptados por el compilador y produzcan los resultados esperados.


%---------------------------------------------------------------------------------
% Descripción y Justificación del Problema a Resolver ----------------------------
%---------------------------------------------------------------------------------

\section{Descripción y Justificación del Problema a Resolver}\label{sec:descr}

\subsection*{Descripción del Problema}

El problema planteado consiste en apoyar a un profesional del campo de la estadística matemática en el diseño y construcción de un lenguaje de programación de propósito específico, orientado a facilitar el análisis de datos mediante la técnica de clasificación de dos vías y la aplicación de pruebas estadísticas del tipo Friedman. 

El lenguaje debe construirse bajo el paradigma imperativo, integrando estructuras de control básicas como secuencia, selección e iteración. Además, debe permitir la definición y manipulación de variables, arreglos y matrices, elementos esenciales para el tratamiento de grandes volúmenes de datos estadísticos. 

Se requiere diseñar una gramática libre de contexto (GLC) en notación E-BNF que describa de forma clara y completa la sintaxis del lenguaje, así como desarrollar los diagramas de sintaxis correspondientes. Igualmente, se debe detallar la interpretación semántica de cada producción de la gramática y plasmarla en diagramas UML, asegurando una implementación coherente mediante el uso de YACC (o sus equivalentes en Java o Python).  

Finalmente, se deben construir casos de prueba representativos para validar el correcto funcionamiento del lenguaje, tanto en su fase de análisis sintáctico como en la ejecución de las acciones semánticas.

\subsection*{Justificación}

El desarrollo de un lenguaje de propósito específico enfocado en el análisis estadístico de dos vías y pruebas de Friedman surge de la necesidad de automatizar procesos estadísticos repetitivos y complejos que, realizados manualmente, resultan propensos a errores y demandan un alto consumo de tiempo y recursos.

Al contar con un lenguaje adaptado a las particularidades de este campo, se facilita la tarea de los investigadores y analistas estadísticos, permitiéndoles escribir programas que describan de forma directa sus experimentos y análisis, sin necesidad de recurrir a lenguajes de propósito general que requieren más tiempo de desarrollo y validación.

Asimismo, este proyecto fortalece las competencias de los estudiantes en la construcción de compiladores, abarcando desde el diseño de gramáticas libres de contexto hasta la implementación de analizadores léxicos y sintácticos, integrando conocimientos de teoría de lenguajes, análisis léxico y semántico, y automatización de tareas mediante herramientas como FLEX y YACC.

%---------------------------------------------------------------------------------
% Diseño de la solución ---------------------------------------------------------
%---------------------------------------------------------------------------------
\section{Diseño solucion}

\subsection{Parser}

El diseño del parser se centra en robustecer el análisis sintáctico, mejorar la flexibilidad del lenguaje y facilitar el mantenimiento futuro. Se abordan cuatro áreas clave: el manejo de retornos de función, la consolidación de expresiones, la flexibilización de estructuras de datos y el reporte de errores.


\subsubsection{Manejo de Retornos de Función}
Para cumplir con el requisito de que las llamadas a funciones puedan ser utilizadas como valores, se expanden las reglas gramaticales de asignación y de impresión.

\begin{itemize}
    \item \textbf{Problema:} Actualmente, una llamada a función como \texttt{miFuncion()} es una sentencia independiente, pero no puede ser parte de una asignación (\texttt{x = miFuncion();}) o una impresión (\texttt{imprimir(miFuncion());}).
    
    \item \textbf{Solución de Diseño:} Se modifican las reglas de producción para que acepten \texttt{func\_call} como un elemento válido a la derecha de un operador de asignación y como argumento de la función \texttt{imprimir}.
\end{itemize}

\begin{lstlisting}[style=mypython, caption={Modificación de la regla \texttt{p\_print} para aceptar llamadas a función.}]
def p_print(p):
    """
    print : PALABCLAVE DELIM (string | ID | func_call) DELIM
    """
    # ... Lógica de validación ...
    p[0] = ASTInterpreter.PrintNode(p[3])
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption={Expansión de la regla \texttt{p\_var\_declaration} para inicializar con el retorno de una función.}]
def p_var_declaration(p):
    """
    var_declaration : datatype ID OPASI ( expression | func_call | ... )
    """
    # ... Lógica de validación ...
\end{lstlisting}


\subsubsection{Consolidación de Reglas de Expresión}
Para reducir la redundancia y simplificar la gramática, se introduce una regla que agrupa los elementos atómicos del lenguaje.

\begin{itemize}
    \item \textbf{Problema:} Reglas como \texttt{string}, \texttt{number}, \texttt{boolean}, e \texttt{ID} se repiten en múltiples producciones, lo que dificulta el mantenimiento.
    
    \item \textbf{Solución de Diseño:} Se crea una nueva regla no terminal, \texttt{primary\_expression}, que agrupa todos estos elementos. Las reglas de nivel superior referenciarán a \texttt{primary\_expression}, simplificando la gramática.
\end{itemize}

\begin{lstlisting}[style=mypython, caption={Definición de la regla de consolidación \texttt{primary\_expression}.}]
def p_primary_expression(p):
    """
    primary_expression : number
                       | string
                       | boolean
                       | ID
                       | func_call
                       | obj_func_call
                       | iterables
    """
    p[0] = p[1]
\end{lstlisting}


\subsubsection{Flexibilidad en Listas y Colecciones}
Las reglas actuales para la creación de listas son demasiado restrictivas. El nuevo diseño permite la creación de colecciones con tipos de datos mixtos.

\begin{itemize}
    \item \textbf{Problema:} Las reglas como \texttt{ints\_array} o \texttt{strings\_array} impiden la creación de listas heterogéneas como \texttt{[10, "hola", verdadero]}.
    
    \item \textbf{Solución de Diseño:} Se eliminan las reglas de array por tipo. Se reutiliza la regla \texttt{arg\_list}, que maneja secuencias de expresiones mixtas, para definir el contenido de los iterables. La validación semántica de tipos se delega a la fase de interpretación.
\end{itemize}

\begin{lstlisting}[style=mypython, caption={Regla simplificada para iterables que permite elementos mixtos.}]
def p_iterables(p):
    """
    iterables : OPACC arg_list OPACC  // Para listas [...]
              | DELIM arg_list DELIM  // Para conjuntos {...}
    """
    # ... Lógica de validación de delimitadores ...
    p[0] = list(p[2])
\end{lstlisting}

\subsubsection{Mejora en el Reporte de Errores Sintácticos}
El diseño se apoya en validaciones explícitas dentro de las reglas gramaticales para proporcionar retroalimentación precisa al programador.

\begin{itemize}
    \item \textbf{Problema:} Un error genérico como ``Error sintáctico cerca de...'' es poco útil.
    
    \item \textbf{Solución de Diseño:} Se mantiene la práctica de insertar comprobaciones dentro de las reglas. Al verificar palabras clave y delimitadores, se lanzan excepciones \texttt{SyntaxError} con mensajes descriptivos.
\end{itemize}

\begin{lstlisting}[style=mypython, caption={Ejemplo de validación explícita en la regla para el bucle \texttt{Mientras}.}]
def p_while_statement(p):
    """
    while_statement : PALABCLAVE DELIM rel_expression DELIM block
    """
    if p[1] != 'Mientras':
        raise SyntaxError("Bucle debe iniciar con 'Mientras'")
    if p[2] != '(' or p[4] != ')':
        raise SyntaxError("Sintaxis: Mientras (expresion) { ... }")
    
    p[0] = Nodes.WhileNode(p[3], p[5])
\end{lstlisting}


\subsection{Analizador Semántico}\label{sec:dis}

La solución se estructura en las siguientes capas semánticas:

\begin{enumerate}
  \item \textbf{Construcción del AST a partir de PLY}  
    \begin{itemize}
      \item Cada regla de la gramática en \texttt{parser.py} (por ejemplo \texttt{p\_program}, \texttt{p\_main\_declaration}, \texttt{p\_func\_declaration}, \ldots) invoca constructores de nodos definidos en \texttt{ASTInterpreter.py}.
      \item Clases de nodo principales:
        \begin{itemize}
          \item \texttt{ProgramNode()} (\texttt{p\_program})
          \item \texttt{MainNode(statements)} (\texttt{p\_main\_declaration})
          \item \texttt{FuncNode(datatypes, ID, params, body)} (\texttt{p\_func\_declaration})
          \item \texttt{PrintNode(expression)} (\texttt{p\_print})
          \item \texttt{VariableDeclaration(datatype, ID, expression)} y \texttt{GenericVariableDeclNode(datatype, ID, arg\_list)} (\texttt{p\_var\_declaration})
          \item Asignaciones: \texttt{VariableAssignationSimple} y \texttt{VariableAssignationItemAccess} (\texttt{p\_var\_assignation})
          \item Expresiones: \texttt{RelExpressionNode}, \texttt{AritExpressionNode}, \texttt{MinusNode}, \texttt{LogExpressionNode}
          \item Control de flujo: \texttt{SelectionNode}, \texttt{ForNode}, \texttt{WhileNode}
          \item Funciones de orden superior: \texttt{LambdaNode}, \texttt{ObjFunctionCall}, \texttt{FunctionCall}
        \end{itemize}
      \item El parser invoca \texttt{parser.parse()} para obtener un AST puro, sin ejecutar nada.
    \end{itemize}

  \item \textbf{Manejo de entorno y alcances (\texttt{Server} y utilidades)}  
    \begin{itemize}
      \item La clase \texttt{Server} (en \texttt{lexer.py}) mantiene:
        \begin{itemize}
          \item \texttt{env['data\_local']} y \texttt{env['data']} para variables locales y globales.
          \item \texttt{env['stack']} y \texttt{env['program\_iterator']} para gestionar llamadas/retornos de funciones.
        \end{itemize}
      \item Ciclo de vida de ejecución:
        \begin{itemize}
          \item \texttt{Server.start\_program()} crea variables de \texttt{Principal} con \texttt{create\_variables(...)} y arranca \texttt{program\_action()}.
          \item \texttt{Server.program\_action()} itera \texttt{program\_iterator}, invocando \texttt{.eval(env)} en cada nodo.
          \item \texttt{Server.swicth\_function(ID)} salva el estado actual en \texttt{env['stack']} y crea un nuevo \texttt{data\_local}.
        \end{itemize}
      \item Búsqueda y definición de variables:
        \begin{itemize}
          \item \texttt{get\_variable\_from\_env(env, ID)} busca en \texttt{data\_local} y \texttt{data}.
          \item \texttt{create\_variables(dict, env)} y \texttt{update\_variables(...)} usan la lógica de \texttt{VariableDeclaration.eval} y \texttt{handle\_\*} para validar y convertir valores.
        \end{itemize}
    \end{itemize}

  \item \textbf{Chequeo de tipos y coherencia semántica}  
    \begin{itemize}
      \item Mapeos de tipos:
        \begin{itemize}
          \item Python→lenguaje: \texttt{python\_type\_to\_language\_type\_list()}, \texttt{python\_type\_to\_language\_type\_string()}.
          \item Lenguaje→Python: \texttt{language\_type\_to\_python\_type()}, \texttt{language\_type\_list\_to\_python\_type()}.
        \end{itemize}
      \item Conversión y validación de valores en \texttt{.eval(env)} de nodos:
        \begin{itemize}
          \item Básicos: \texttt{handle\_type\_a\_conversion(datatype\_str, value)}.
          \item Contenedores anidados: \texttt{handle\_nested\_containers(datatypes, value)}.
          \item Contenedores especiales (\texttt{Diccionario}, \texttt{Matriz}, \ldots): \texttt{handle\_special\_containers()}, \texttt{handle\_diccionario\_case()}, \texttt{handle\_tda\_containers\_case()}.
        \end{itemize}
      \item Cada nodo implementa un método \texttt{eval(env)} que:
        \begin{itemize}
          \item Evalúa subexpresiones o recupera literales.
          \item Llama a las utilidades \texttt{handle\_\*} para garantizar tipado.
          \item Usa \texttt{create\_variables} / \texttt{update\_variables} para declaraciones y asignaciones.
          \item Lanza \texttt{ValueError}, \texttt{TypeError} o \texttt{IndexError} en errores semánticos.
        \end{itemize}
    \end{itemize}

  \item \textbf{Generación de código intermedio (próximos pasos)}  
    \begin{itemize}
      \item Actualmente el intérprete ejecuta directamente el AST. Para un compilador:
        \begin{itemize}
          \item Añadir \texttt{emit()} en cada clase de nodo (ej. en \texttt{AritExpressionNode}, \texttt{VariableDeclaration}, …).
          \item Mantener una lista global \texttt{QuadList} y un generador de temporales \texttt{new\_temp()}.
          \item Ejemplo prototipo:
            \begin{lstlisting}[language=Python]
def emit(self):
    left = self.termino1.addr
    right = self.termino2.addr
    self.addr = new_temp()
    QuadList.append((self.oparit, left, right, self.addr))
            \end{lstlisting}
        \end{itemize}
    \end{itemize}

  \item \textbf{Manejo de errores semánticos}  
    \begin{itemize}
      \item Reemplazar \texttt{print()} por excepciones o acumulación en una lista de errores.
      \item En el lexer/parser: \texttt{t\_error} y \texttt{p\_error} deben lanzar o registrar errores.
      \item En la fase semántica: capturar errores de tipo e índice en los \texttt{.eval(env)} y al final presentar un informe único.
      \item Eventual implementación de \texttt{semanticError(msg,pos)} para centralizar mensajes.
    \end{itemize}
\end{enumerate}



%---------------------------------------------------------------------------------
% Código Fuente ---------------------------------------------------------
%---------------------------------------------------------------------------------

\section{Código Fuente}\label{sec:cod}

El código fuente completo de este modelo se encuentra adjunto como 
(taller2.zip)
y disponible en el repositorio GitHub del proyecto:

\begin{center}
\url{https://github.com/JavierTarazona06/LP02_Tareas/tree/main/taller2/code}
\end{center}

%---------------------------------------------------------------------------------
% Manual Usuario ---------------------------------------------------------
%---------------------------------------------------------------------------------

\section{Manual Usuario}\label{sec:man_u}

\subsection{Clonar el repositorio}

Abra una terminal y ejecute:

\begin{verbatim}
git clone https://github.com/JavierTarazona06/LP02_Tareas.git
cd LP02_Tareas/taller2/code
\end{verbatim}

\subsection{Crear y activar el entorno virtual}

\textbf{En Windows:}
\begin{verbatim}
python -m venv .venv
.venv\Scripts\activate
\end{verbatim}

\textbf{En macOS o Linux:}
\begin{verbatim}
python3 -m venv .venv
source .venv/bin/activate
\end{verbatim}

\subsection{Instalar dependencias}
El proyecto requiere únicamente el paquete \texttt{PLY} (Python Lex-Yacc):

\begin{verbatim}
pip install ply
\end{verbatim}

O si tienes un archivo \texttt{requirements.txt}:
\begin{verbatim}
pip install -r requirements.txt
\end{verbatim}

\subsection{Ejecutar un programa}
Coloca tu archivo fuente (por ejemplo, \texttt{tests/prototype1.txt}) en la carpeta \texttt{code/tests/}.

Para ejecutar el parser e intérprete sobre tu archivo fuente:
\begin{verbatim}
python parser.py tests/prototype1.txt
\end{verbatim}

Si no especificas un archivo, por defecto se usará \texttt{tests/prototype1.txt}.

\subsection{¿Qué hace el comando?}
\begin{itemize}
    \item Analiza el archivo fuente.
    \item Construye el árbol de sintaxis abstracta (AST).
    \item Ejecuta el programa.
    \item Guarda el AST en un archivo binario (\texttt{ast\_guardado.pickle}).
\end{itemize}

\subsection{Ejemplo de archivo fuente}
Crea un archivo \texttt{tests/prototype1.txt} con el siguiente contenido:

\begin{verbatim}
Func Vacio Principal() {
    Entero x = 5;
    imprimir(x);
}
\end{verbatim}

\subsection{Notas adicionales}
\begin{itemize}
    \item Si ves errores de módulos, asegúrate de tener el entorno virtual activado y las dependencias instaladas.
    \item El intérprete imprime los resultados en la consola.
\end{itemize}


%---------------------------------------------------------------------------------
% Manual Técnico ---------------------------------------------------------
%---------------------------------------------------------------------------------

\section{Manual Técnico}\label{sec:man_t}

\subsection{Estructura del Proyecto}

El proyecto está organizado de la siguiente manera:

\begin{itemize}
    \item \textbf{code/} \\ Carpeta principal del código fuente.
    \begin{itemize}
        \item \texttt{lexer.py}: Analizador léxico (tokens y reglas).
        \item \texttt{parser.py}: Analizador sintáctico y punto de entrada principal.
        \item \texttt{ASTInterpreter.py}: Definición de nodos AST y lógica de interpretación/ejecución.
        \item \texttt{TDA.py}: Tipos de datos abstractos usados por el lenguaje.
        \item \texttt{semantic\_analyzer.py}: (Opcional) Análisis semántico adicional.
        \item \texttt{tests/}: Casos de prueba de entrada.
    \end{itemize}
    \item \textbf{latex/} \\ Documentación y archivos LaTeX.
    \item \textbf{latex/grammar.ebnf} \\ \textbf{Archivo adjunto con los diagramas y la gramática formal del lenguaje.}
\end{itemize}

\subsection*{Nota sobre los diagramas de gramática}

Los diagramas y la descripción formal de la gramática del lenguaje se encuentran en el archivo adjunto \texttt{grammar.ebnf} dentro de la carpeta \texttt{latex/}.

\subsection{Archivos Principales}

\begin{description}
    \item[lexer.py] Define los tokens, expresiones regulares y reglas de análisis léxico usando PLY.
    \item[parser.py] Define la gramática, las reglas de producción y la construcción del AST. También contiene el punto de entrada para ejecutar el parser e intérprete.
    \item[ASTInterpreter.py] Implementa los nodos del AST, el entorno de ejecución y la lógica de evaluación de cada nodo (declaraciones, expresiones, ciclos, funciones, etc.).
    \item[TDA.py] Implementa los tipos de datos abstractos (arreglos, conjuntos, diccionarios, matrices, etc.) usados por el lenguaje.
    \item[tests/prototypeX.txt] Archivos de prueba con programas de ejemplo escritos en el lenguaje diseñado.
\end{description}

\subsection{Flujo de Ejecución}

\begin{enumerate}
    \item El usuario ejecuta \texttt{parser.py} con un archivo fuente como argumento.
    \item El archivo fuente es leído y procesado por el lexer y parser (PLY), construyendo el AST.
    \item El nodo raíz del AST es evaluado, lo que inicia la ejecución del programa.
    \item El entorno de ejecución (\texttt{env}) gestiona variables, funciones y el flujo de control.
    \item Los resultados y salidas se imprimen en consola.
\end{enumerate}

\subsection{Extensión y Mantenimiento}

\begin{itemize}
    \item Para agregar nuevas construcciones al lenguaje, modifica \texttt{parser.py} (reglas de gramática) y \texttt{ASTInterpreter.py} (nodos y lógica de evaluación).
    \item Para agregar nuevos tipos de datos, implementa la clase correspondiente en \texttt{TDA.py} y actualiza las reglas de conversión en \texttt{ASTInterpreter.py}.
    \item Para depuración, puedes imprimir el AST generado o el entorno de ejecución en cualquier punto del código.
    \item Los errores sintácticos y semánticos se reportan con mensajes claros en consola.
\end{itemize}

\subsection{Dependencias}

\begin{itemize}
    \item \textbf{Python 3.10+}
    \item \textbf{PLY} (Python Lex-Yacc)
\end{itemize}

\subsection{Notas de Depuración}

\begin{itemize}
    \item Si el parser no reconoce una construcción, revisa la definición de tokens y reglas en \texttt{lexer.py} y \texttt{parser.py}.
    \item Si el AST no se comporta como esperas, imprime los nodos y el entorno en \texttt{ASTInterpreter.py}.
    \item Usa los archivos de prueba en \texttt{tests/} para validar cambios y nuevas funcionalidades.
\end{itemize}


%---------------------------------------------------------------------------------
% Experimentación ---------------------------------------------------------
%---------------------------------------------------------------------------------

\section{Experimentación}\label{sec:exp}

\subsection{Análisis de resultados}

\subsubsection{Escenario 1: Código correcto completo}



\subsubsection{Escenario 2: Código con errores léxicos}


\subsubsection{Escenario 3:  Anidamientos Complejos }



\section{Conclusiones}

\begin{itemize}
    \item La construcción de un lenguaje de propósito específico orientado a la estadística permite optimizar procesos de análisis de datos, reduciendo tiempos y minimizando errores humanos.
    
    \item El diseño de una gramática libre de contexto, acompañado de diagramas de sintaxis y UML, proporciona una base sólida para la implementación de compiladores eficientes y mantenibles.
    
    \item La integración de herramientas como FLEX y YACC en la construcción del compilador refuerza habilidades prácticas en el desarrollo de analizadores léxicos y sintácticos, permitiendo validar la coherencia entre la sintaxis y la semántica del lenguaje propuesto.
  
\end{itemize}


\section{Referencias}
\renewcommand{\refname}{}

\begin{thebibliography}{9}

%---------------------------------------------------------------------------------
% Referencias, aunque creo que mejor deberíamos usar un .bib y llamarlas desde ahí, es más facil ---------------------------------------------------------
%---------------------------------------------------------------------------------

\bibitem{ref} \label{ref:lexPy1} J. R. Levine, T. Mason, and D. 
Brown, “Lex \& Yacc,” 2nd ed., O’Reilly \& Associates, 1992.

\bibitem{ref} \label{ref:lexPy2}  D. M. Beazley, “PLY (Python Lex‐Yacc)
Manual,” Version 3.11, 2023. [Online]. Available: https://www.dabeaz.com/ply/.

\bibitem{ref} \label{ref:rachas} J.~E.~Ortiz~Triviño, ``Lenguaje para 
  procesamiento de rachas,'' Documento interno, Universidad Nacional de 
    Colombia, enviado por correo electrónico, 6 de mayo de 2025.

\end{thebibliography}

\end{document}
